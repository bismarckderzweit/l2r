{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "from multiprocessing import Pool\n",
    "from utils.rerank import write_average, rerank_ndcg,write_tau,write_ratio\n",
    "from utils.readdata import get_microsoft_data, rewrite\n",
    "from utils.separate_set import separate_set\n",
    "from utils.explainer_tools import rand_row, evaluate, get_rankedduculist, get_set_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X):\n",
    "    A = []\n",
    "    # this part is for getting shapley values\n",
    "    scorefile_path = temp_path + 'scorefile_shapk_{}.txt'.format(tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "    restore_path = temp_path + 'restore_shapk_{}.txt'.format(tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "    rewrite(X, tmp_test_y_query, tmp_test_Query, restore_path)\n",
    "    args = ['java', '-jar', 'RankLib-2.12.jar', '-rank', restore_path, '-load', model,\n",
    "            '-indri', scorefile_path]\n",
    "    subprocess.check_output(args, stderr=subprocess.STDOUT, timeout=2000)\n",
    "\n",
    "    # rerank the scorefile according the original index\n",
    "    scorefile_data = ''.join(sorted(open(scorefile_path), key=lambda s: s.split()[1], reverse=False))\n",
    "    with open(scorefile_path, 'w') as f:\n",
    "        f.write(scorefile_data)\n",
    "    with open(scorefile_path, 'r') as f:\n",
    "        for line in f:\n",
    "            A.append(float(line.split()[-2]))\n",
    "\n",
    "    # reset the index to be original otherwise can not get the right NDCG\n",
    "    restore_context = open(restore_path, 'r').readlines()\n",
    "    with open(restore_path, 'w') as f:\n",
    "        for lineindex in range(len(restore_context)):\n",
    "            split = restore_context[lineindex].split()\n",
    "            split[1] = 'qid:{}'.format(tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "            newline = ''\n",
    "            for i in range(len(split)):\n",
    "                newline += (split[i] + ' ')\n",
    "            f.write(newline + '\\n')\n",
    "\n",
    "    A = np.array(A)\n",
    "    return A\n",
    "\n",
    "\n",
    "def loop_query(query_index):\n",
    "    \"\"\"\n",
    "    loop for a query, get scores of the samples of this query and rank them according to the scores\n",
    "    :param query_index: the index of query\n",
    "    :return: ranklist file, delta NDCG file\n",
    "    \"\"\"\n",
    "    # get data for this query\n",
    "    global tmp_test_data\n",
    "    global tmp_test_y_query\n",
    "    global tmp_test_Query\n",
    "    tmp_test_data =test_data[query_index]\n",
    "    tmp_test_y_query = test_y_query[query_index]\n",
    "    tmp_test_Query = test_Query[query_index]\n",
    "    query_id = tmp_test_y_query[0].split(':')[-1].split()[0]\n",
    "\n",
    "    # calculate the scores for the q-d pairs\n",
    "    scores = score(tmp_test_data).reshape(-1,1)\n",
    "    restore_path = temp_path + 'restore_shapk_{}.txt'.format(query_id)\n",
    "    scorefile_path = temp_path + 'scorefile_shapk_{}.txt'.format(query_id)\n",
    "\n",
    "    # reranking the test_data according to the scores and get the list of ranking\n",
    "    test_data_score = np.append(tmp_test_data,scores,axis=1)\n",
    "    ranked_test_data = (test_data_score[(-test_data_score[:,-1]).argsort()])[:,:-1]\n",
    "    rankedduculist1 = get_rankedduculist(scores, query_index,q_d_len)\n",
    "    NDCG_before =evaluate(model,restore_path)\n",
    "\n",
    "\n",
    "    NDCG_file_name = NDCGdata_path + '{}_SHAP{}_complement'.format(dataname,k) + modelname + '.txt'\n",
    "    ranklist_file = NDCGdata_path + '{}_ranklist_SHAP{}_complement'.format(dataname,k) + modelname + '.txt'\n",
    "    all_features = [i for i in range(46)]\n",
    "    top_k_idx =feature_set[query_index]\n",
    "    complement_idx = list(set(all_features) - set(top_k_idx[:5]))\n",
    "    features_to_change = tmp_test_data.copy()\n",
    "    features_to_change[:,complement_idx] = expected_value[complement_idx]\n",
    "    # get scores of the changed features\n",
    "    scores2 = score(features_to_change).reshape(-1,1)\n",
    "    NDCG_after = evaluate(model,restore_path)\n",
    "    delta_NDCG = abs(float(NDCG_before) - float(NDCG_after))\n",
    "    if float(NDCG_before)  == 0:\n",
    "        ratio_NDCG = 0\n",
    "    else:\n",
    "        ratio_NDCG = delta_NDCG/float(NDCG_before) \n",
    "\n",
    "    rankedduculist2 = get_rankedduculist(scores2, query_index,q_d_len)\n",
    "    tau, p_value = stats.kendalltau(rankedduculist1, rankedduculist2)\n",
    "    os.remove(scorefile_path)\n",
    "    os.remove(restore_path)\n",
    "    with open(NDCG_file_name,'a') as NDCG_FILE:\n",
    "        NDCG_line = tmp_test_y_query[0].split(':')[-1]+'  ' + \\\n",
    "                    'changed feature:'+ str(complement_idx)+'  '+'kendalltau='+str(round(tau,4))+ '  '+'ratioNDCG:'+ str(round(ratio_NDCG,4))+'  '+'delta_NDCG ='+'  '+str(delta_NDCG)+\"\\n\"\n",
    "        NDCG_FILE.write(NDCG_line)\n",
    "    with open(ranklist_file, 'a') as ranklist:\n",
    "        ranklist_line = tmp_test_y_query[0].split(':')[-1] + '  ' + 'ranklist before:' + str(\n",
    "            rankedduculist1) + '  ' + 'ranklist after:' + '  ' + str(rankedduculist2) + \"\\n\"\n",
    "        ranklist.write(ranklist_line)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #parameters to be set\n",
    "    model_path = 'model/'\n",
    "    model = model_path + 'coordinateascent_model.txt'\n",
    "    k_set = [1,5]  # k: shap k, we select the top kexample to do analysis\n",
    "\n",
    "    for f in range(1,2):\n",
    "    # the path of data\n",
    "    #datapath = 'MQ2008/Fold1/'\n",
    "        datapath = 'MQ2008/Fold{}/'.format(f)\n",
    "        #datapath = 'MSLR-WEB10K/Fold{}/'.format(f)\n",
    "        train_path = datapath + 'train.txt'\n",
    "        test_path = datapath + 'test.txt'\n",
    "        modelname = model.split(\"_\")[0].split(\"/\")[-1]\n",
    "        dataname = datapath.split('/')[0] +'_'+ datapath.split('/')[1].split('Fold')[1]\n",
    "\n",
    "        # saving path and save files\n",
    "        NDCGdata_path = 'NDCGdata/'\n",
    "        temp_path = 'temp_data_shapk_complement/'\n",
    "\n",
    "\n",
    "        # get train data and test data\n",
    "        X_train, y_query_train, Query_train = get_microsoft_data(train_path)\n",
    "        X_train = np.array(X_train)\n",
    "        X_test, y_query_test, Query_test = get_microsoft_data(test_path)\n",
    "        X_test = np.array(X_test)\n",
    "        expected_value = np.mean(X_train, axis=0)\n",
    "\n",
    "        # separate the test set\n",
    "        test_data, test_y_query, test_Query, q_d_len = separate_set(y_query_test, X_test, Query_test)\n",
    "        \n",
    "        for k in k_set:\n",
    "            resultfile_NDCG = 'resultfile/' + '{}_{}_shapk{}_complement_NDCG.txt'.format(dataname,modelname,k)\n",
    "            resultfile_tau = 'resultfile/' + '{}_{}_shapk{}_complement_tau.txt'.format(dataname,modelname,k)\n",
    "            resultfile_ratio =  'resultfile/' + '{}_{}_shapk{}_complement_ratio.txt'.format(dataname,modelname,k)\n",
    "            old_NDCG_file = 'F1/'+'{}_SHAP{}_10features_threshold0{}.txt'.format(dataname,k,modelname)\n",
    "            feature_set = []\n",
    "            with open(old_NDCG_file,'r') as oldfile:\n",
    "                for line in oldfile:\n",
    "                    features = ((line.split('[')[-1].split(']')[0])).split()\n",
    "                    if len(features) <10:break\n",
    "                    this_feature_set = [int(features[i]) for i in range(10)]\n",
    "                    feature_set.append(this_feature_set)\n",
    "            with Pool(10) as p:\n",
    "                print(p.map(loop_query, [query_index for query_index in range(len(test_data))]))\n",
    "    \n",
    "\n",
    "            NDCG_file_name = NDCGdata_path + '{}_SHAP{}_complement'.format(dataname,k) + modelname + '.txt'\n",
    "            ranklist_file = NDCGdata_path + '{}_ranklist_SHAP{}_complement'.format(dataname,k) + modelname + '.txt'\n",
    "            rerank_ndcg(NDCG_file_name)\n",
    "            rerank_ndcg(ranklist_file)\n",
    "            tau = write_tau(NDCG_file_name)\n",
    "            NDCG = write_average(NDCG_file_name)\n",
    "            ratio = write_ratio(NDCG_file_name)\n",
    "            with open(resultfile_NDCG,'a') as NDCG_result:\n",
    "                NDCG_result_line  = str(NDCG) + \"\\n\"\n",
    "                NDCG_result.write(NDCG_result_line)\n",
    "            with open(resultfile_tau,'a') as tau_result:\n",
    "                tau_result_line  = str(tau) + \"\\n\" \n",
    "                tau_result.write(tau_result_line)\n",
    "            with open(resultfile_ratio,'a') as ratio_result:\n",
    "                ratio_result_line  = str(ratio) + \"\\n\" \n",
    "                ratio_result.write(ratio_result_line)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
