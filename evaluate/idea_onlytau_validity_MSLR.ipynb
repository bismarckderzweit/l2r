{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "os.chdir(os.path.expanduser('..'))\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "from multiprocessing import Pool\n",
    "from utils.rerank import write_average, rerank_ndcg, rerank_matrix,write_tau,write_ratio\n",
    "from utils.readdata import get_microsoft_data, rewrite\n",
    "from utils.separate_set import separate_set\n",
    "from utils.explainer_tools import rand_row, evaluate, get_pairsname, get_rankedduculist, get_set_cover\n",
    "from itertools import combinations\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X):\n",
    "    \"\"\"\n",
    "    The first if branch is training data, the next is for the single test data. First calling the subprocess of ranklib\n",
    "    to get the scores, then rerank the scorefile according the original index. We also have to delete the produced\n",
    "    files which used by the subprocess.\n",
    "    :param X: input feature matrix\n",
    "    :return: scores of q-d pairs\n",
    "    \"\"\"\n",
    "    A = []\n",
    "    scorefile_path = temp_path + 'scorefile_ideavalidity_{}.txt'.format(tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "    restore_path = temp_path + 'restore_ideavalidity_{}.txt'.format(tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "    rewrite(X, tmp_test_y_query, tmp_test_Query, restore_path)\n",
    "    args = ['java', '-jar', 'RankLib-2.12.jar', '-rank', restore_path, '-load', model,\n",
    "            '-indri', scorefile_path]\n",
    "    subprocess.check_output(args, stderr=subprocess.STDOUT)\n",
    "\n",
    "    # rerank the scorefile according the original index\n",
    "    scorefile_data = ''.join(sorted(open(scorefile_path), key=lambda s: s.split()[1], reverse=False))\n",
    "    with open(scorefile_path, 'w') as f:\n",
    "        f.write(scorefile_data)\n",
    "    with open(scorefile_path, 'r') as f:\n",
    "        for line in f:\n",
    "            A.append(float(line.split()[-2]))\n",
    "\n",
    "    # reset the index to be original otherwise can not get the right NDCG\n",
    "    restore_context = open(restore_path, 'r').readlines()\n",
    "    with open(restore_path, 'w') as f:\n",
    "        for lineindex in range(len(restore_context)):\n",
    "            split = restore_context[lineindex].split()\n",
    "            split[1] = 'qid:{}'.format(tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "            newline = ''\n",
    "            for i in range(len(split)):\n",
    "                newline += (split[i] + ' ')\n",
    "            f.write(newline + '\\n')\n",
    "    A = np.array(A)\n",
    "    return A\n",
    "\n",
    "def chunks(arr, m):\n",
    "    n = int(math.ceil(len(arr) / float(m)))\n",
    "    return [arr[i:i + n] for i in range(0, len(arr), n)]\n",
    "\n",
    "def  loop_chunk(chunk_id):\n",
    "    features_to_change_set = []\n",
    "    all_features = [i for i in range(tmp_test_data.shape[1])]\n",
    "    for i in range(top_k_idx_list[chunk_id].shape[0]):\n",
    "        top_k_idx = top_k_idx_list[chunk_id][i]\n",
    "        complement_idx = list(set(all_features) - set(top_k_idx))\n",
    "        features_to_change = tmp_test_data.copy()\n",
    "        features_to_change[:,complement_idx] = expected_value[complement_idx]\n",
    "        features_to_change_set.append(features_to_change)\n",
    "\n",
    "\n",
    "    with open(temp_path+'changed_list_WMATRIXideavalidity{}.txt'.format(chunk_id),'w') as f:\n",
    "        for i in range(len(features_to_change_set)*len(tmp_test_data)):\n",
    "            line = \"\"\n",
    "            line += \"0 qid:{} \".format(str(i))\n",
    "            for j in range(len(tmp_test_data[0])):\n",
    "                line += ((str(j+1))+\":\"+str(features_to_change_set[i//len(tmp_test_data)][i%len(tmp_test_data)][j])+\" \")\n",
    "            line += \"\\n\"\n",
    "            f.write(line)\n",
    "    args = ['java', '-jar', 'RankLib-2.12.jar', '-rank', temp_path+'changed_list_WMATRIXideavalidity{}.txt'.format(chunk_id), '-load', model,\n",
    "            '-indri', temp_path+'changed_list_WMATRIXideavalidity_score{}.txt'.format(chunk_id)]\n",
    "    subprocess.check_output(args, stderr=subprocess.STDOUT)\n",
    "    A = ''.join(sorted(open(temp_path+'changed_list_WMATRIXideavalidity_score{}.txt'.format(chunk_id)), key=lambda s: int(s.split()[0]), reverse=False))\n",
    "    with open(temp_path+'changed_list_WMATRIXideavalidity_score{}.txt'.format(chunk_id),'w') as f:\n",
    "        f.write(A)\n",
    "    changed_list_score = []\n",
    "    with open(temp_path+'changed_list_WMATRIXideavalidity_score{}.txt'.format(chunk_id),'r') as f:\n",
    "        for line in f:\n",
    "            changed_list_score.append(float(line.split()[-2]))\n",
    "    changed_list_score =  [changed_list_score[i:i + tmp_test_data.shape[0]] for i in range(0, len(changed_list_score), tmp_test_data.shape[0])]   \n",
    "    os.remove(os.path.join(temp_path, 'changed_list_WMATRIXideavalidity{}.txt'.format(chunk_id)))\n",
    "    os.remove(os.path.join(temp_path, 'changed_list_WMATRIXideavalidity_score{}.txt'.format(chunk_id))) \n",
    "    tau_set = []\n",
    "\n",
    "\n",
    "    for i in range(len(changed_list_score)):\n",
    "        this_score_list = np.array(changed_list_score[i]).reshape(-1, 1)\n",
    "        rankedduculist2 = get_rankedduculist(this_score_list, query_index, q_d_len)\n",
    "        tau, p_value = stats.kendalltau(rankedduculist1, rankedduculist2)\n",
    "        tau_set.append(tau)\n",
    "    best_tau =  max(tau_set)\n",
    "    best_index = tau_set.index(best_tau)\n",
    "    best_top_k_idx = top_k_idx_list[chunk_id][best_index]\n",
    "    best_tau_dict[str(best_top_k_idx)] = best_tau\n",
    "    with open(temp_best_file, 'a') as temp_best:\n",
    "        temp_best_line = str(best_top_k_idx)+' '+ str(round(best_tau,4)) + \"\\n\"\n",
    "        temp_best.write(temp_best_line)\n",
    "    with open(idea_log,'a') as tau_log:\n",
    "        tau_log_line = ' '.join(map(str, tau_set)) + '\\n'\n",
    "        tau_log.write(tau_log_line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #parameters to be set\n",
    "    dataset = 'mslr'\n",
    "    model_path = 'MSLR-WEB10K_model/'\n",
    "    dataset_path = 'MSLR-WEB10K/'\n",
    "    modelname_index = 1    \n",
    "    model_set  =['LambdaMART_model.txt','RankBoost_model.txt','LinearRegression_model.txt']\n",
    "    \n",
    "    for MODEL in model_set:\n",
    "        model = model_path + MODEL\n",
    "    \n",
    "        for f in range(1,2):\n",
    "        # the path of data\n",
    "            \n",
    "            datapath = dataset_path+'Fold{}/'.format(f)\n",
    "            train_path = datapath + 'train.txt'\n",
    "            test_path = datapath + 'test.txt'\n",
    "            modelname = model.split(\"_\")[modelname_index].split(\"/\")[-1]\n",
    "            dataname = datapath.split('/')[0] +'_'+ datapath.split('/')[1].split('Fold')[1]\n",
    "            temp_path = 'temp_file/'\n",
    "            \n",
    "            \n",
    "            # get train data and test data\n",
    "            \n",
    "            X_train, y_query_train, Query_train = get_microsoft_data(train_path)\n",
    "            X_train = np.array(X_train)\n",
    "            X_test, y_query_test, Query_test = get_microsoft_data(test_path)\n",
    "            X_test = np.array(X_test)\n",
    "            expected_value = np.mean(X_train, axis=0)\n",
    "            \n",
    "\n",
    "            # separate the test set\n",
    "            test_data, test_y_query, test_Query, q_d_len = separate_set(y_query_test, X_test, Query_test)\n",
    "            \n",
    "            resultfile_idea = 'ideafeatures/' + '{}_{}_validity_ideafeatures.txt'.format(dataname,modelname)\n",
    "            idea_log =  'ideafeatures/' + '{}_{}_validity_idealog.txt'.format(dataname,modelname)\n",
    "            A = [i for i in range(500)]\n",
    "            B = A[::5]\n",
    "            for i in B:\n",
    "                query_index = i\n",
    "                best_tau_dict = {}\n",
    "                tmp_test_data =test_data[i]\n",
    "                tmp_test_y_query = test_y_query[i]\n",
    "                tmp_test_Query = test_Query[i]\n",
    "                query_id = tmp_test_y_query[0].split(':')[-1].split()[0]\n",
    "                restore_path = temp_path +  'restore_ideavalidity_{}.txt'.format(query_id)\n",
    "                scorefile_path = temp_path + 'scorefile_ideavalidity_{}.txt'.format(query_id)\n",
    "                temp_best_file = temp_path + 'temp_bestvalidity_{}.txt'.format(query_id)\n",
    "                scores = score(tmp_test_data).reshape(-1, 1)\n",
    "                test_data_score = np.append(tmp_test_data,scores,axis=1)\n",
    "                ranked_test_data = np.array((test_data_score[(-test_data_score[:,-1]).argsort()])[:,:-1])\n",
    "                rankedduculist1 = get_rankedduculist(scores, i, q_d_len)\n",
    "                top_k_idx_list = np.array([c for c in combinations(range(tmp_test_data.shape[1]), 3)])\n",
    "                top_k_idx_list = chunks(top_k_idx_list,20)\n",
    "                with open(idea_log,'a') as tau_log:\n",
    "                    firstline = 'qid:' + str(query_id) + '\\n'\n",
    "                    tau_log.write(firstline)\n",
    "                \n",
    "                with Pool(20) as p:\n",
    "                    print(p.map(loop_chunk, [chunk_index for chunk_index in range(20)]))\n",
    "                with open(temp_best_file,'r') as temp_best:\n",
    "                    for line in temp_best:\n",
    "                        features = ((line.split('[')[1].split(']')[0])).split()\n",
    "                        this_feature_set = [int(features[i].rstrip(',')) for i in range(len(features))]\n",
    "                        best_tau_dict[str(this_feature_set)] = float(line.split()[-1])\n",
    "                    \n",
    "                true_best_top_k_idx, true_best_tau = max(best_tau_dict.items(),key=lambda x:x[1])\n",
    "                resultfile_idea = 'ideafeatures/' + '{}_{}_validity_ideafeatures.txt'.format(dataname,modelname)\n",
    "                \n",
    "                \n",
    "                with open(resultfile_idea, 'a') as IDEA_FILE:\n",
    "                    idea_line =  tmp_test_y_query[0].split(':')[-1]+'  ' \\\n",
    "                                + 'changed feature:'+ true_best_top_k_idx +' '+'kendalltau='+str(round(true_best_tau,4))+ \"\\n\"\n",
    "                    IDEA_FILE.write(idea_line)\n",
    "                os.remove(scorefile_path)\n",
    "                os.remove(restore_path)\n",
    "                os.remove(temp_best_file)\n",
    "            rerank_ndcg(resultfile_idea)\n",
    "            tau = write_tau(resultfile_idea)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
