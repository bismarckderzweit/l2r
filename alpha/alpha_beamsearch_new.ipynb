{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "os.chdir(os.path.expanduser('..'))\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "from multiprocessing import Pool\n",
    "from utils.rerank import write_average, rerank_ndcg, rerank_matrix,write_tau,write_ratio,write_tau2,write_ratio2,write_average2\n",
    "from utils.readdata import get_microsoft_data, rewrite\n",
    "from utils.separate_set import separate_set\n",
    "from utils.explainer_tools import rand_row, evaluate, get_pairsname, get_rankedduculist, small_get_pairsname,get_set_cover,get_set_cover_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X):\n",
    "    \"\"\"\n",
    "    The first if branch is training data, the next is for the single test data. First calling the subprocess of ranklib\n",
    "    to get the scores, then rerank the scorefile according the original index. We also have to delete the produced\n",
    "    files which used by the subprocess.\n",
    "    :param X: input feature matrix\n",
    "    :return: scores of q-d pairs\n",
    "    \"\"\"\n",
    "    A = []\n",
    "    scorefile_path = temp_path + 'scorefile_alphanew{}_{}.txt'.format(alpha,tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "    restore_path = temp_path + 'restore_alphanew{}_{}.txt'.format(alpha,tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "    rewrite(X, tmp_test_y_query, tmp_test_Query, restore_path)\n",
    "    args = ['java', '-jar', 'RankLib-2.12.jar', '-rank', restore_path, '-load', model,\n",
    "            '-indri', scorefile_path]\n",
    "    subprocess.check_output(args, stderr=subprocess.STDOUT,timeout = 2000)\n",
    "\n",
    "    with open(scorefile_path, 'r') as f:\n",
    "        for line in f:\n",
    "            A.append(float(line.split()[-2]))\n",
    "\n",
    "    # reset the index to be original otherwise can not get the right NDCG\n",
    "    restore_context = open(restore_path, 'r').readlines()\n",
    "    with open(restore_path, 'w') as f:\n",
    "        for lineindex in range(len(restore_context)):\n",
    "            split = restore_context[lineindex].split()\n",
    "            split[1] = 'qid:{}'.format(tmp_test_y_query[0].split(':')[-1].split()[0])\n",
    "            newline = ''\n",
    "            for i in range(len(split)):\n",
    "                newline += (split[i] + ' ')\n",
    "            f.write(newline + '\\n')\n",
    "    A = np.array(A)\n",
    "    return A\n",
    "\n",
    "\n",
    "def loop_query(query_index):\n",
    "    \"\"\"\n",
    "    loop for a query, get scores of the samples of this query and rank them according to the scores\n",
    "    :param query_index: the index of query\n",
    "    :return: ranklist file, matrix file, delta NDCG file\n",
    "    \"\"\"\n",
    "    # get data for this query\n",
    "    global tmp_test_data\n",
    "    global tmp_test_y_query\n",
    "    global tmp_test_Query\n",
    "    tmp_test_data =test_data[query_index]\n",
    "    tmp_test_y_query = test_y_query[query_index]\n",
    "    tmp_test_Query = test_Query[query_index]\n",
    "    query_id = tmp_test_y_query[0].split(':')[-1].split()[0]\n",
    "\n",
    "    # calculate the scores for the q-d pairs\n",
    "    restore_path = temp_path +  'restore_alphanew{}_{}.txt'.format(alpha,query_id)\n",
    "    scorefile_path = temp_path + 'scorefile_alphanew{}_{}.txt'.format(alpha,query_id)\n",
    "    scores = score(tmp_test_data).reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # reranking the test_data according to the scores and get the list of ranking\n",
    "    test_data_score = np.append(tmp_test_data,scores,axis=1)\n",
    "    ranked_test_data = np.array((test_data_score[(-test_data_score[:,-1]).argsort()])[:,:-1])\n",
    "    rankedduculist1 = get_rankedduculist(scores, query_index, q_d_len)\n",
    "    NDCG_before = evaluate(model, restore_path)\n",
    "\n",
    "    # get pairsname\n",
    "    global pairsname\n",
    "    if q_d_len[query_index] >= 11:\n",
    "        pairnumbers = 50\n",
    "        pairsname = get_pairsname(ranked_test_data, pairnumbers)\n",
    "    else:\n",
    "        pairsname = small_get_pairsname(ranked_test_data)\n",
    "    original_pairsname = pairsname.copy()\n",
    "    def get_score_matrix1(feature_matrix):\n",
    "        changed_list = []\n",
    "        for i in range(feature_matrix.shape[0]):\n",
    "            for m in range(tmp_test_data.shape[1]):\n",
    "                temp = expected_value.copy()\n",
    "                temp[m] = feature_matrix[i,m]\n",
    "                temp[top_k_idx] = feature_matrix[i,top_k_idx]\n",
    "                changed_list.append(temp)\n",
    "        changed_list = np.array(changed_list)\n",
    "        with open(temp_path+'changed_list_newsufficiency{}.txt'.format(query_index),'w') as f:\n",
    "            for i in range(feature_matrix.shape[0]*tmp_test_data.shape[1]):\n",
    "                line = \"\"\n",
    "                line += \"0 qid:{} \".format(str(i))\n",
    "                for j in range(len(changed_list[i])):\n",
    "                    line += ((str(j+1))+\":\"+str(changed_list[i][j])+\" \")\n",
    "                line += '#docid = GX008-86-4444840 inc = 1 prob = 0.086622 ' + \"\\n\"\n",
    "                f.write(line)\n",
    "        args = ['java', '-jar', 'RankLib-2.12.jar', '-rank', temp_path+'changed_list_newsufficiency{}.txt'.format(query_index), '-load', model,\n",
    "                '-indri', temp_path+'changed_list_newsufficiency_score{}.txt'.format(query_index)]\n",
    "        subprocess.check_output(args, stderr=subprocess.STDOUT)\n",
    "        A = ''.join(sorted(open(temp_path+'changed_list_newsufficiency_score{}.txt'.format(query_index)), key=lambda s: int(s.split()[0]), reverse=False))\n",
    "        with open(temp_path+'changed_list_newsufficiency_score{}.txt'.format(query_index),'w') as f:\n",
    "            f.write(A)\n",
    "        changed_list_score = []\n",
    "        with open(temp_path+'changed_list_newsufficiency_score{}.txt'.format(query_index),'r') as f:\n",
    "            for line in f:\n",
    "                changed_list_score.append(float(line.split()[-2]))\n",
    "        changed_list_score =  [changed_list_score[i:i + tmp_test_data.shape[1]] for i in range(0, len(changed_list_score), tmp_test_data.shape[1])]   \n",
    "        os.remove(os.path.join(temp_path, 'changed_list_newsufficiency{}.txt'.format(query_index)))\n",
    "        os.remove(os.path.join(temp_path, 'changed_list_newsufficiency_score{}.txt'.format(query_index))) \n",
    "        return changed_list_score\n",
    "    \n",
    "    def get_score_matrix2(feature_matrix):\n",
    "        changed_list = []\n",
    "        for i in range(feature_matrix.shape[0]):\n",
    "            temp =  feature_matrix[i].copy()\n",
    "            for m in range(tmp_test_data.shape[1]):\n",
    "                temp2 = temp.copy()\n",
    "                temp2[m] = expected_value[m]\n",
    "                temp2[top_k_idx] = expected_value[top_k_idx]\n",
    "                changed_list.append(temp2)\n",
    "        changed_list = np.array(changed_list)\n",
    "        with open(temp_path+'changed_list_newcompleteness{}.txt'.format(query_index),'w') as f:\n",
    "            for i in range(feature_matrix.shape[0]*X_test.shape[1]):\n",
    "                line = \"\"\n",
    "                line += \"0 qid:{} \".format(str(i))\n",
    "                for j in range(len(changed_list[i])):\n",
    "                    line += ((str(j+1))+\":\"+str(changed_list[i][j])+\" \")\n",
    "                line += '#docid = GX008-86-4444840 inc = 1 prob = 0.086622 ' + \"\\n\"\n",
    "                f.write(line)\n",
    "        args = ['java', '-jar', 'RankLib-2.12.jar', '-rank', temp_path+'changed_list_newcompleteness{}.txt'.format(query_index), '-load', model,\n",
    "                '-indri', temp_path+'changed_list_newcompleteness_score{}.txt'.format(query_index)]\n",
    "        subprocess.check_output(args, stderr=subprocess.STDOUT, timeout = 2000)\n",
    "        A = ''.join(sorted(open(temp_path+'changed_list_newcompleteness_score{}.txt'.format(query_index)), key=lambda s: int(s.split()[0]), reverse=False))\n",
    "        with open(temp_path+'changed_list_newcompleteness_score{}.txt'.format(query_index),'w') as f:\n",
    "            f.write(A)\n",
    "        changed_list_score = []\n",
    "        with open(temp_path+'changed_list_newcompleteness_score{}.txt'.format(query_index),'r') as f:\n",
    "            for line in f:\n",
    "                changed_list_score.append(float(line.split()[-2]))\n",
    "        changed_list_score =  [changed_list_score[i:i + tmp_test_data.shape[1]] for i in range(0, len(changed_list_score), tmp_test_data.shape[1])]   \n",
    "        os.remove(os.path.join(temp_path, 'changed_list_newcompleteness{}.txt'.format(query_index)))\n",
    "        os.remove(os.path.join(temp_path, 'changed_list_newcompleteness_score{}.txt'.format(query_index))) \n",
    "        return changed_list_score\n",
    "        \n",
    "        \n",
    "    def get_matrix(ranked_test_data):\n",
    "        score_values1 = get_score_matrix1(ranked_test_data)\n",
    "        score_values2 = get_score_matrix2(ranked_test_data)\n",
    "        matrix = []\n",
    "        for i in range(len(pairsname)):\n",
    "            index1 = int(pairsname[i][1])\n",
    "            index2 = int(pairsname[i][-1])\n",
    "          \n",
    "            row = [(alpha*round((score_values1[index1-1][j]-score_values1[index2-1][j]),10)+(1-alpha)*round((score_values2[index2-1][j]\n",
    "                                                                                                            -score_values2[index1-1][j]),10) )for j in range(tmp_test_data.shape[1])]\n",
    "            matrix.append(row)\n",
    "        return matrix       \n",
    "\n",
    "    global top_k_idx_set \n",
    "    top_k_idx_set = []\n",
    "    pairs_set = []\n",
    "    matrix_set = []\n",
    "    for i in range(beam_size):\n",
    "        top_k_idx = []\n",
    "        pairsname = original_pairsname.copy()\n",
    "        matrix  = get_matrix(ranked_test_data)\n",
    "        temp_index = get_set_cover_beam(matrix)[i]\n",
    "        temp2_index = temp_index\n",
    "        top_k_idx.append(temp_index)\n",
    "        temp_tmp_test_data = tmp_test_data.copy()\n",
    "\n",
    "        # get the left 9 indexes\n",
    "        for i in range(9):\n",
    "            temp_tmp_test_data[:,temp_index] = expected_value[temp_index]\n",
    "            scores_temp = score(temp_tmp_test_data).reshape(-1,1)\n",
    "            rankedduculist_temp = get_rankedduculist(scores_temp, query_index, q_d_len)\n",
    "            changedpairs = []\n",
    "\n",
    "            this_feature=[x[temp2_index] for x in matrix]\n",
    "            if len([x for x in this_feature if x >0]) == 0: break\n",
    "            threshold = sum([x for x in this_feature if x >0])/len([x for x in this_feature if x >0])\n",
    "            drop_pairs = np.array([this_feature[i] >= threshold for i in range(len(this_feature)) ]).astype(int)   \n",
    "\n",
    "            for i in range(len(drop_pairs)):\n",
    "                if drop_pairs[i] == 1:\n",
    "                    changedpairs.append(pairsname[i])\n",
    "            if len(changedpairs) >= len(pairsname): break  \n",
    "            for i in range(len(changedpairs)):     \n",
    "                if changedpairs[i] in pairsname:\n",
    "                    pairsname.remove(changedpairs[i])\n",
    "                    \n",
    "            temp_matrix  = get_matrix(ranked_test_data)\n",
    "\n",
    "            #delect the features we already selected\n",
    "            all_features = [i for i in range(tmp_test_data.shape[1])]\n",
    "            left_idx = list(set(all_features) - set(top_k_idx))\n",
    "            temp_matrix = list(map(list, zip(*temp_matrix)))    \n",
    "            matrix = []\n",
    "            for i in left_idx:\n",
    "                matrix.append(temp_matrix[i])\n",
    "            matrix = list(map(list, zip(*matrix)))  \n",
    "            temp_index= get_set_cover(matrix)\n",
    "            temp2_index = temp_index\n",
    "            temp_top_k_idx = top_k_idx.copy()\n",
    "            for i in temp_top_k_idx:\n",
    "                if i <= temp_index:\n",
    "                    temp_index +=1\n",
    "                    while temp_index in temp_top_k_idx:\n",
    "                        temp_top_k_idx.remove(temp_index)\n",
    "                        temp_index +=1\n",
    "\n",
    "            top_k_idx.append(temp_index)\n",
    "        top_k_idx_set.append(top_k_idx)\n",
    "        pairs_set.append(pairsname)\n",
    "        matrix_set.append(matrix)\n",
    "        \n",
    "    def feature_k_loop():\n",
    "        NDCG_file_name = NDCGdata_path + '{}_alphabeamsearchnew{}_5features'.format(dataname,alpha) + modelname + '.txt'\n",
    "        NDCG_file_matrix = NDCGdata_path + '{}_alphabeamsearchnew{}_matrix_5features'.format(dataname,alpha)  + modelname + '.txt'\n",
    "        ranklist_file = NDCGdata_path + '{}_ranklist_alphabeamsearchnew{}_5features'.format(dataname,alpha)  + modelname + '.txt'\n",
    "        complement_NDCG_file_name =  NDCGdata_path + '{}_alphabeamsearchnew{}_complement'.format(dataname,alpha) + modelname + '.txt'\n",
    "        complement_ranklist_file = NDCGdata_path + '{}_ranklist_alphabeamsearchnew{}_complement'.format(dataname,alpha)  + modelname + '.txt'\n",
    "       \n",
    "        def get_selected_index(feature_number):\n",
    "            best_tau = 1\n",
    "            tau_set= []\n",
    "            tau2_set= []\n",
    "            ratio_NDCG_set = []\n",
    "            ratio_NDCG2_set = []\n",
    "            delta_NDCG_set = []\n",
    "            delta_NDCG2_set = []\n",
    "            rankedduculist2_set = []\n",
    "            rankedduculist3_set = []\n",
    "            tau_conbination_set = []\n",
    "            for i in range(beam_size):\n",
    "                features_to_change = tmp_test_data.copy()\n",
    "                top_k_idx = top_k_idx_set[i]\n",
    "                all_features = [i for i in range(tmp_test_data.shape[1])]\n",
    "                complement_idx = list(set(all_features) - set(top_k_idx[:feature_number]))\n",
    "                if len(top_k_idx)<= feature_number:\n",
    "                    feature_number = len(top_k_idx)\n",
    "                features_to_change[:,top_k_idx[0:feature_number]] = expected_value[top_k_idx[0:feature_number]]\n",
    "                restore_path = temp_path +  'restore_alphanew{}_{}.txt'.format(alpha,query_id)\n",
    "                scorefile_path = temp_path + 'scorefile_alphanew{}_{}.txt'.format(alpha,query_id)\n",
    "                # get scores of the changed features\n",
    "                scores2 = score(features_to_change).reshape(-1,1)\n",
    "                rankedduculist2 = get_rankedduculist(scores2, query_index, q_d_len)\n",
    "                NDCG_after = evaluate(model, restore_path)\n",
    "                delta_NDCG = abs(float(NDCG_before) - float(NDCG_after))\n",
    "                if float(NDCG_before)  == 0:\n",
    "                    ratio_NDCG = 0\n",
    "                else:\n",
    "                    ratio_NDCG = delta_NDCG/float(NDCG_before) \n",
    "                tau, p_value = stats.kendalltau(rankedduculist1, rankedduculist2)\n",
    "\n",
    "                tau_set.append(tau)\n",
    "                ratio_NDCG_set.append(ratio_NDCG)\n",
    "                delta_NDCG_set.append(delta_NDCG)\n",
    "                rankedduculist2_set.append(rankedduculist2)\n",
    "\n",
    "                features_to_change = tmp_test_data.copy()\n",
    "                features_to_change[:,complement_idx] = expected_value[complement_idx]\n",
    "                # get scores of the changed features\n",
    "                scores3 = score(features_to_change).reshape(-1,1)\n",
    "                rankedduculist3 = get_rankedduculist(scores3, query_index, q_d_len)\n",
    "                NDCG_after2 = evaluate(model, restore_path)\n",
    "                delta_NDCG2 = abs(float(NDCG_before) - float(NDCG_after2))\n",
    "                if float(NDCG_before)  == 0:\n",
    "                    ratio_NDCG2 = 0\n",
    "                else:\n",
    "                    ratio_NDCG2 = delta_NDCG2/float(NDCG_before) \n",
    "                tau2, p_value2 = stats.kendalltau(rankedduculist1, rankedduculist3)\n",
    "\n",
    "                tau2_set.append(tau2)\n",
    "                ratio_NDCG2_set.append(ratio_NDCG2)\n",
    "                delta_NDCG2_set.append(delta_NDCG2)\n",
    "                rankedduculist3_set.append(rankedduculist3)\n",
    "                tau_conbination = 1/(tau + 2) + tau2\n",
    "                tau_conbination_set.append(tau_conbination)\n",
    "\n",
    "            selected_index = tau_conbination_set.index(max(tau_conbination_set))\n",
    "            best_pairsname = pairs_set[selected_index]\n",
    "            best_matrix = matrix_set[selected_index]\n",
    "            return selected_index, tau_set,tau2_set, ratio_NDCG_set ,ratio_NDCG2_set, delta_NDCG_set, delta_NDCG2_set, rankedduculist3_set, rankedduculist2_set\n",
    "        selected_index, tau_set,tau2_set, ratio_NDCG_set ,ratio_NDCG2_set, delta_NDCG_set, delta_NDCG2_set, rankedduculist3_set, rankedduculist2_set = get_selected_index(5)\n",
    "\n",
    "        best_pairsname = pairs_set[selected_index]\n",
    "        best_matrix = matrix_set[selected_index]\n",
    "        \n",
    "        with open(NDCG_file_name, 'a') as NDCG_FILE:\n",
    "            NDCG_line =  tmp_test_y_query[0].split(':')[-1]+'  ' \\\n",
    "                        + 'changed feature:'+ str(top_k_idx_set[selected_index])+' '+'kendalltau='+str(round(tau_set[selected_index],4)) + '  '+'ratioNDCG:'+ str(round(ratio_NDCG_set[selected_index],4))+ '  ' + 'pairnames:'+' '+str(best_pairsname) + \\\n",
    "                        '   ' + 'delta_NDCG ='+'  '+str(round(delta_NDCG_set[selected_index],4))+ \"\\n\"\n",
    "            NDCG_FILE.write(NDCG_line)\n",
    "        with open(NDCG_file_matrix, 'a') as matrix_FILE:\n",
    "            matrix_line = 'matrix for {}'.format(tmp_test_y_query[0].split(':')[-1].split()[0]) \\\n",
    "                          + '  ' + str(best_matrix) + '  ' + \"\\n\"\n",
    "            matrix_FILE.write(matrix_line)\n",
    "        \n",
    "        with open(ranklist_file, 'a') as ranklist:\n",
    "            ranklist_line = tmp_test_y_query[0].split(':')[-1] + '  ' + 'ranklist before:' + str(\n",
    "                rankedduculist1) + '  ' + 'ranklist after:' + '  ' + str(rankedduculist2_set[selected_index]) + \"\\n\"\n",
    "            ranklist.write(ranklist_line)\n",
    "        if dataset == 'mq2008': \n",
    "            all_features = [i for i in range(tmp_test_data.shape[1])]\n",
    "            complement_idx = list(set(all_features) - set(top_k_idx_set[selected_index][:5]))      \n",
    "            with open(complement_NDCG_file_name, 'a') as NDCG_FILE:\n",
    "                NDCG_line =  tmp_test_y_query[0].split(':')[-1]+'  ' \\\n",
    "                            + 'changed feature:'+ str(complement_idx)+' '+'kendalltau='+str(round(tau2_set[selected_index],4)) + '  '+'ratioNDCG:'+ str(round(ratio_NDCG2_set[selected_index],4))+ '  ' + \\\n",
    "                            '   ' + 'delta_NDCG ='+'  '+str(round(delta_NDCG2_set[selected_index],4))+ \"\\n\"\n",
    "                NDCG_FILE.write(NDCG_line)\n",
    "\n",
    "            with open(complement_ranklist_file, 'a') as ranklist:\n",
    "                ranklist_line = tmp_test_y_query[0].split(':')[-1] + '  ' + 'ranklist before:' + str(\n",
    "                    rankedduculist1) + '  ' + 'ranklist after:' + '  ' + str(rankedduculist3_set[selected_index]) + \"\\n\"\n",
    "                ranklist.write(ranklist_line)    \n",
    "        \n",
    "        NDCG_file_name = NDCGdata_path + '{}_alphabeamsearchnew{}_10features'.format(dataname,alpha) + modelname + '.txt'\n",
    "        NDCG_file_matrix = NDCGdata_path + '{}_alphabeamsearchnew{}_matrix_10features'.format(dataname,alpha)  + modelname + '.txt'\n",
    "        ranklist_file = NDCGdata_path + '{}_ranklist_alphabeamsearchnew{}_10features'.format(dataname,alpha)  + modelname + '.txt'\n",
    "        \n",
    "        \n",
    "        features_to_change = tmp_test_data.copy()\n",
    "        if dataset == 'mq2008': \n",
    "            selected_index2 = selected_index\n",
    "            top_k_idx = top_k_idx_set[selected_index2]\n",
    "            features_to_change[:,top_k_idx[0:10]] = expected_value[top_k_idx[0:10]]\n",
    "            restore_path = temp_path +  'restore_alphanew{}_{}.txt'.format(alpha,query_id)\n",
    "            scorefile_path = temp_path + 'scorefile_alphanew{}_{}.txt'.format(alpha,query_id)\n",
    "            # get scores of the changed features\n",
    "            scores2 = score(features_to_change).reshape(-1,1)\n",
    "            rankedduculist2 = get_rankedduculist(scores2, query_index, q_d_len)\n",
    "            NDCG_after = evaluate(model, restore_path)\n",
    "            delta_NDCG = abs(float(NDCG_before) - float(NDCG_after))\n",
    "            if float(NDCG_before)  == 0:\n",
    "                ratio_NDCG = 0\n",
    "            else:\n",
    "                ratio_NDCG = delta_NDCG/float(NDCG_before) \n",
    "            tau, p_value = stats.kendalltau(rankedduculist1, rankedduculist2)\n",
    "            with open(NDCG_file_name, 'a') as NDCG_FILE:\n",
    "                NDCG_line =  tmp_test_y_query[0].split(':')[-1]+'  ' \\\n",
    "                            + 'changed feature:'+ str(top_k_idx)+' '+'kendalltau='+str(round(tau,4)) + '  '+'ratioNDCG:'+ str(round(ratio_NDCG,4))+ '  ' + 'pairnames:'+' '+str(pairsname) + \\\n",
    "                            '   ' + 'delta_NDCG ='+'  '+str(round(delta_NDCG,4))+ \"\\n\"\n",
    "                NDCG_FILE.write(NDCG_line)\n",
    "            with open(NDCG_file_matrix, 'a') as matrix_FILE:\n",
    "                matrix_line = 'matrix for {}'.format(tmp_test_y_query[0].split(':')[-1].split()[0]) \\\n",
    "                              + '  ' + str(best_matrix) + '  ' + \"\\n\"\n",
    "                matrix_FILE.write(matrix_line)\n",
    "            with open(ranklist_file, 'a') as ranklist:\n",
    "                ranklist_line = tmp_test_y_query[0].split(':')[-1] + '  ' + 'ranklist before:' + str(\n",
    "                    rankedduculist1) + '  ' + 'ranklist after:' + '  ' + str(rankedduculist2) + \"\\n\"\n",
    "                ranklist.write(ranklist_line)\n",
    "                \n",
    "        if  dataset =='mslr':\n",
    "            selected_index2, tau_set2,tau2_set2, ratio_NDCG_set2 ,ratio_NDCG2_set2, delta_NDCG_set2, delta_NDCG2_set2, rankedduculist3_set2, rankedduculist2_set2 = get_selected_index(10)   \n",
    "            all_features = [i for i in range(tmp_test_data.shape[1])]\n",
    "            top_k_idx = top_k_idx_set[selected_index2]\n",
    "            best_pairsname = pairs_set[selected_index]\n",
    "            best_matrix = matrix_set[selected_index]\n",
    "            complement_idx1 = list(set(all_features) - set(top_k_idx_set[selected_index][:5]))   \n",
    "            complement_idx2 = list(set(all_features) - set(top_k_idx_set[selected_index2])) \n",
    "            with open(NDCG_file_name, 'a') as NDCG_FILE:\n",
    "                NDCG_line =  tmp_test_y_query[0].split(':')[-1]+'  ' \\\n",
    "                            + 'changed feature:'+ str(top_k_idx_set[selected_index2])+' '+'kendalltau='+str(round(tau_set2[selected_index2],4)) + '  '+'ratioNDCG:'+ str(round(ratio_NDCG_set2[selected_index2],4))+ '  ' + 'pairnames:'+' '+str(best_pairsname) + \\\n",
    "                            '   ' + 'delta_NDCG ='+'  '+str(round(delta_NDCG_set2[selected_index2],4))+ \"\\n\"\n",
    "                NDCG_FILE.write(NDCG_line)\n",
    "            with open(NDCG_file_matrix, 'a') as matrix_FILE:\n",
    "                matrix_line = 'matrix for {}'.format(tmp_test_y_query[0].split(':')[-1].split()[0]) \\\n",
    "                              + '  ' + str(best_matrix) + '  ' + \"\\n\"\n",
    "                matrix_FILE.write(matrix_line)\n",
    "            with open(ranklist_file, 'a') as ranklist:\n",
    "                ranklist_line = tmp_test_y_query[0].split(':')[-1] + '  ' + 'ranklist before:' + str(\n",
    "                    rankedduculist1) + '  ' + 'ranklist after:' + '  ' + str(rankedduculist2_set2[selected_index2]) + \"\\n\"\n",
    "            with open(complement_NDCG_file_name, 'a') as NDCG_FILE:\n",
    "                NDCG_line =  tmp_test_y_query[0].split(':')[-1]+'  ' \\\n",
    "                            + 'changed 5features:'+ str(complement_idx1)+' '+'kendalltau='+str(round(tau2_set[selected_index],4)) + '  '+'ratioNDCG:'+ str(round(ratio_NDCG2_set[selected_index],4)) + \\\n",
    "                            '   ' + 'delta_NDCG ='+'  '+str(round(delta_NDCG2_set[selected_index],4))+' '+ 'changed 10features:'+ str(complement_idx2)+' '+'kendalltau2='+str(round(tau2_set2[selected_index2],4)) + '  '+'ratioNDCG2:'+ str(round(ratio_NDCG2_set2[selected_index2],4)) + \\\n",
    "                            '   ' + 'delta_NDCG2 ='+'  '+str(round(delta_NDCG2_set2[selected_index2],4))+ \"\\n\"\n",
    "                NDCG_FILE.write(NDCG_line)\n",
    "\n",
    "            with open(complement_ranklist_file, 'a') as ranklist:\n",
    "                ranklist_line = tmp_test_y_query[0].split(':')[-1] + '  ' + 'ranklist before:' + str(\n",
    "                    rankedduculist1) + '  ' + 'ranklist after:' + '  ' + str(rankedduculist3_set2[selected_index2]) + \"\\n\"\n",
    "                ranklist.write(ranklist_line)    \n",
    "\n",
    "    feature_k_loop()        \n",
    "    os.remove(scorefile_path)\n",
    "    os.remove(restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/wang/shap_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f98c2a4ac202>\", line 53, in <module>\n",
      "    loop_query(0)\n",
      "  File \"<ipython-input-2-921c9e4cf25a>\", line 185, in loop_query\n",
      "    temp_matrix  = get_matrix(ranked_test_data)\n",
      "  File \"<ipython-input-2-921c9e4cf25a>\", line 142, in get_matrix\n",
      "    score_values2 = get_score_matrix2(ranked_test_data)\n",
      "  File \"<ipython-input-2-921c9e4cf25a>\", line 121, in get_score_matrix2\n",
      "    line += ((str(j+1))+\":\"+str(changed_list[i][j])+\" \")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wang/shap_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wang/shap_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/wang/shap_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/wang/shap_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib64/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib64/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib64/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib64/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/wang/shap_env/lib64/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/wang/shap_env/lib64/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/wang/shap_env/lib64/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #parameters to be set\n",
    "    dataset = 'mslr'\n",
    "    beam_size = 5\n",
    "    if dataset == 'mq2008':\n",
    "        model_path = 'model/'\n",
    "        dataset_path = 'MQ2008/'\n",
    "        modelname_index = 0\n",
    "        model_set  =['LambdaMART_model.txt','Ranknet_model.txt','Linearregression_model.txt']\n",
    "    else:\n",
    "        model_path = 'MSLR-WEB10K_model/'\n",
    "        dataset_path = 'MSLR-WEB10K/'\n",
    "        modelname_index = 1    \n",
    "        model_set  =['LambdaMART_model.txt','RankBoost_model.txt','LinearRegression_model.txt']\n",
    "       \n",
    "    for f in range(1,2):\n",
    "        # the path of data\n",
    "        datapath =dataset_path+'Fold{}/'.format(f)\n",
    "        train_path = datapath + 'train.txt'\n",
    "        test_path = datapath + 'test.txt'\n",
    "        dataname = datapath.split('/')[0] +'_'+ datapath.split('/')[1].split('Fold')[1]\n",
    "        # saving path and save files\n",
    "        NDCGdata_path = 'logs/'\n",
    "        temp_path = 'temp_file/'\n",
    "        \n",
    "        # get train data and test data\n",
    "        X_train, y_query_train, Query_train = get_microsoft_data(train_path)\n",
    "        X_train = np.array(X_train)\n",
    "        X_test, y_query_test, Query_test = get_microsoft_data(test_path)\n",
    "        X_test = np.array(X_test)\n",
    "        expected_value = np.mean(X_train, axis=0)\n",
    "\n",
    "        # separate the test set\n",
    "        test_data, test_y_query, test_Query, q_d_len = separate_set(y_query_test, X_test, Query_test)\n",
    "        \n",
    "        for alpha in np.arange(0.1,1.0,0.2):\n",
    "            alpha  = round(alpha,4)\n",
    "            for MODEL in model_set:\n",
    "                model = model_path + MODEL\n",
    "                modelname = model.split(\"_\")[modelname_index].split(\"/\")[-1]\n",
    "                resultfile_NDCG = 'resultfile/' + '{}_{}_alphabeamsearchnew_NDCG.txt'.format(dataname,modelname)\n",
    "                resultfile_tau = 'resultfile/' + '{}_{}_alphabeamsearchnew_tau.txt'.format(dataname,modelname)\n",
    "                resultfile_ratio =  'resultfile/' + '{}_{}_alphabeamsearchnew_ratio.txt'.format(dataname,modelname)\n",
    "                complement_resultfile_NDCG = 'resultfile/' + '{}_{}_alphabeamsearchnewcomplement_NDCG.txt'.format(dataname,modelname)\n",
    "                complement_resultfile_tau = 'resultfile/' + '{}_{}_alphabeamsearchnewcomplement_tau.txt'.format(dataname,modelname)\n",
    "                complement_resultfile_ratio =  'resultfile/' + '{}_{}_alphabeamsearchnewcomplement_ratio.txt'.format(dataname,modelname)\n",
    "\n",
    "                with Pool(1) as p:\n",
    "                    if dataset == 'mq2008':\n",
    "                        print(p.map(loop_query, [query_index for query_index in range(len(test_data))])) \n",
    "                        \n",
    "                    else:\n",
    "                        print(p.map(loop_query, [query_index for query_index in range(500)]))\n",
    "                        \n",
    "                        \n",
    "                for feature_number in (5,10):\n",
    "                    NDCG_file_name = NDCGdata_path + '{}_alphabeamsearchnew{}_{}features'.format(dataname,alpha,feature_number) + modelname + '.txt'\n",
    "                    NDCG_file_matrix = NDCGdata_path + '{}_alphabeamsearchnew{}_matrix_{}features'.format(dataname,alpha,feature_number)  + modelname + '.txt'\n",
    "                    ranklist_file = NDCGdata_path + '{}_ranklist_alphabeamsearchnew{}_{}features'.format(dataname,alpha,feature_number)  + modelname + '.txt'\n",
    "                    complement_NDCG_file_name =  NDCGdata_path + '{}_alphabeamsearchnew{}_complement'.format(dataname,alpha) + modelname + '.txt'\n",
    "                    complement_ranklist_file = NDCGdata_path + '{}_ranklist_alphabeamsearchnew{}_complement'.format(dataname,alpha)  + modelname + '.txt'\n",
    "\n",
    "                    rerank_ndcg(NDCG_file_name)\n",
    "                    NDCG =  write_average(NDCG_file_name)\n",
    "                    rerank_ndcg(ranklist_file)\n",
    "                    rerank_matrix(NDCG_file_matrix)\n",
    "                    ratio = write_ratio(NDCG_file_name)\n",
    "                    tau = write_tau(NDCG_file_name)\n",
    "                    with open(resultfile_NDCG, 'a') as NDCG_result:\n",
    "                        NDCG_result_line  = str(NDCG) + \"\\n\"\n",
    "                        NDCG_result.write(NDCG_result_line)\n",
    "                    with open(resultfile_tau,'a') as tau_result:\n",
    "                        tau_result_line  = str(tau) + \"\\n\" \n",
    "                        tau_result.write(tau_result_line)\n",
    "                    with open(resultfile_ratio,'a') as ratio_result:\n",
    "                        ratio_result_line  = str(ratio) + \"\\n\" \n",
    "                        ratio_result.write(ratio_result_line) \n",
    "\n",
    "                if dataset == 'mq2008':\n",
    "                    rerank_ndcg(complement_NDCG_file_name)\n",
    "                    NDCG =  write_average(complement_NDCG_file_name)\n",
    "                    rerank_ndcg(complement_ranklist_file)\n",
    "                    ratio = write_ratio(complement_NDCG_file_name)\n",
    "                    tau = write_tau(complement_NDCG_file_name)\n",
    "                    with open(complement_resultfile_NDCG, 'a') as NDCG_result:\n",
    "                        NDCG_result_line  = str(NDCG) + \"\\n\"\n",
    "                        NDCG_result.write(NDCG_result_line)\n",
    "                    with open(complement_resultfile_tau,'a') as tau_result:\n",
    "                        tau_result_line  = str(tau) + \"\\n\" \n",
    "                        tau_result.write(tau_result_line)\n",
    "                    with open(complement_resultfile_ratio,'a') as ratio_result:\n",
    "                        ratio_result_line  = str(ratio) + \"\\n\" \n",
    "                        ratio_result.write(ratio_result_line)\n",
    "                else:\n",
    "                    rerank_ndcg(complement_NDCG_file_name)\n",
    "                    rerank_ndcg(complement_ranklist_file)\n",
    "                    for feature_number in (5,10):\n",
    "                        NDCG =  write_average2(complement_NDCG_file_name,feature_number)\n",
    "                        ratio = write_ratio2(complement_NDCG_file_name,feature_number)\n",
    "                        tau = write_tau2(complement_NDCG_file_name,feature_number)\n",
    "                        with open(complement_resultfile_NDCG, 'a') as NDCG_result:\n",
    "                            NDCG_result_line  = str(NDCG) + \"\\n\"\n",
    "                            NDCG_result.write(NDCG_result_line)\n",
    "                        with open(complement_resultfile_tau,'a') as tau_result:\n",
    "                            tau_result_line  = str(tau) + \"\\n\" \n",
    "                            tau_result.write(tau_result_line)\n",
    "                        with open(complement_resultfile_ratio,'a') as ratio_result:\n",
    "                            ratio_result_line  = str(ratio) + \"\\n\" \n",
    "                            ratio_result.write(ratio_result_line)   \n",
    "                            \n",
    "            NDCG_file_name_1 = NDCGdata_path + '{}_alphabeamsearchnew_10features'.format(dataname) + modelname + '.txt'\n",
    "            ranklist_file_1 = NDCGdata_path + '{}_ranklist_alphabeamsearchnew_10features'.format(dataname)  + modelname + '.txt'      \n",
    "            NDCG_file_matrix_1 = NDCGdata_path + '{}_alphabeamsearchnew_matrix_10features'.format(dataname)  + modelname + '.txt'\n",
    "            NDCG_file_name_2 = NDCGdata_path + '{}_alphabeamsearchnew_5features'.format(dataname) + modelname + '.txt'\n",
    "            ranklist_file_2 = NDCGdata_path + '{}_ranklist_alphabeamsearchnew_5features'.format(dataname)  + modelname + '.txt'\n",
    "            NDCG_file_matrix_2= NDCGdata_path + '{}_alphabeamsearchnew_matrix_5features'.format(dataname)  + modelname + '.txt'\n",
    "            NDCG_file_name = NDCGdata_path + '{}_alphabeamsearchnew'.format(dataname) + modelname + '.txt'\n",
    "            ranklist_file = NDCGdata_path + '{}_ranklist_alphabeamsearchnew'.format(dataname)  + modelname + '.txt' \n",
    "            NDCG_file_matrix = NDCGdata_path + '{}_alphabeamsearchnew_matrix'.format(dataname)  + modelname + '.txt'\n",
    "            first_part_set = []\n",
    "            second_part_set = []\n",
    "            with open(NDCG_file_name_1,'r') as fa:\n",
    "                for linea in fa:\n",
    "                    first_part = linea.split()[0]+' '+'changed 10features:='+linea.split('changed feature:')[1].split('kendalltau=')[0] +' '+'kendalltau10='+linea.split('kendalltau')[1].split()[0]+' '+'ratioNDCG10:'+linea.split('ratioNDCG:')[1].split()[0]+' '+\\\n",
    "                    'pairnames10: '+linea.split('pairnames: ')[1].split('delta_NDCG')[0]+ 'delta_NDCG10 ='+ linea.split()[-1] + ' '\n",
    "                    first_part_set.append(first_part)\n",
    "                    \n",
    "            with open(NDCG_file_name_2, 'r') as fb:\n",
    "                for lineb in fb:\n",
    "                    second_part = 'changed 5features:='+lineb.split('changed feature:')[1].split('kendalltau=')[0]+ 'kendalltau5='+lineb.split('kendalltau')[1].split()[0]+' '+'ratioNDCG5:'+lineb.split('ratioNDCG:')[1].split()[0]+' '+\\\n",
    "                    'pairnames5: '+lineb.split('pairnames: ')[1].split('delta_NDCG')[0]+ 'delta_NDCG5='+ lineb.split()[-1] + ' '\n",
    "                    second_part_set.append(second_part)\n",
    "                        \n",
    "            with open(NDCG_file_name,'w') as fc:\n",
    "                for i in range(len(first_part_set)):\n",
    "                    fc.write(first_part_set[i])\n",
    "                    fc.write(second_part_set[i]+'\\n')\n",
    "            list_set1 = []\n",
    "            list_set2 = []\n",
    "            with open(ranklist_file_1,'r') as fa:\n",
    "                for linea in fa:\n",
    "                    first_part = linea.split()[0]+' '+'ranklist before:'+linea.split('ranklist before:')[1].split('ranklist after:')[0] +' '+ 'ranklist after10:'+linea.split('ranklist after:')[1].split('\\n')[0]+' '\n",
    "                    list_set1.append(first_part)\n",
    "            with open(ranklist_file_2,'r') as fb:\n",
    "                for lineb in fb:\n",
    "                    second_part ='ranklist after5:'+lineb.split('ranklist after:')[1].split('\\n')[0]\n",
    "                    list_set2.append(second_part)              \n",
    "            with open(ranklist_file,'w') as fc:\n",
    "                for i in range(len(list_set1)):\n",
    "                    fc.write(list_set1[i])\n",
    "                    fc.write(list_set2[i]+'\\n')\n",
    "            matrix_set1 = []\n",
    "            matrix_set2 = []\n",
    "            with open(NDCG_file_matrix_1,'r') as fa:\n",
    "                for linea in fa:\n",
    "                    first_part = ' '.join(linea.split()[:3])+' '+'matrix for 10 features:'+''.join(linea.split()[3:])+' '\n",
    "                    matrix_set1.append(first_part)\n",
    "            with open(NDCG_file_matrix_2,'r') as fb:\n",
    "                for lineb in fb:\n",
    "                    second_part ='matrix for 5 features:'+''.join(lineb.split()[3:])\n",
    "                    matrix_set2.append(second_part)              \n",
    "            with open(NDCG_file_matrix,'w') as fc:\n",
    "                for i in range(len(matrix_set1)):\n",
    "                    fc.write(matrix_set1[i])\n",
    "                    fc.write(matrix_set2[i]+'\\n')    \n",
    "                    \n",
    "                    \n",
    "            os.remove(NDCG_file_name_1) \n",
    "            os.remove(NDCG_file_name_2) \n",
    "            os.remove(ranklist_file_1) \n",
    "            os.remove(ranklist_file_2)    \n",
    "            os.remove(NDCG_file_matrix_1)\n",
    "            os.remove(NDCG_file_matrix_2)              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
